{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T09:40:12.877821688Z",
     "start_time": "2024-02-23T09:38:21.234460895Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "# pick the model you want to use\n",
    "MODEL_NAME = \"pranaydeeps/EXALT-Baseline\"\n",
    "\n",
    "# load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80719199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "import torch\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "\n",
    "class CustomSequenceClassificationExplainer(SequenceClassificationExplainer): # need custom explainer to handle xlm-roberta\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        attribution_type: str = \"lig\",\n",
    "        custom_labels: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__(model, tokenizer)\n",
    "        \n",
    "    def _make_input_reference_token_type_pair(self, input_ids: torch.Tensor, sep_idx: int = 0\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns two tensors indicating the corresponding token types for the `input_ids`\n",
    "        and a corresponding all zero reference token type tensor.\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Tensor of text converted to `input_ids`\n",
    "            sep_idx (int, optional):  Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]\n",
    "        \"\"\"\n",
    "        seq_len = input_ids.size(1)\n",
    "        \n",
    "        if self.model.config.model_type == 'xlm-roberta':\n",
    "            token_type_ids = torch.zeros(seq_len, dtype=torch.int, device=self.device).expand_as(input_ids)\n",
    "        else:\n",
    "            token_type_ids = torch.tensor([0 if i <= sep_idx else 1 for i in range(seq_len)], device=self.device).expand_as(\n",
    "                input_ids\n",
    "            )\n",
    "        ref_token_type_ids = torch.zeros_like(token_type_ids, device=self.device).expand_as(input_ids)\n",
    "\n",
    "        return (token_type_ids, ref_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec11a7adb7b64313",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T09:40:23.277644236Z",
     "start_time": "2024-02-23T09:40:22.968733212Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-token attributions: [('<s>', 0.0), ('‚ñÅMy', 0.0023662683176213025), ('‚ñÅparents', -0.0044839087437695165), ('‚ñÅhat', 0.13700055463100105), ('ed', 0.838185384402232), ('‚ñÅthis', 0.4072319378334896), ('‚ñÅridiculous', 0.2140027340738356), ('‚ñÅmovie', 0.12540142859780076), ('‚ñÅ', 0.1389692863864906), ('.', 0.1788222651295735), ('</s>', 0.0)]\n",
      "Predicted class: LABEL_4\n"
     ]
    }
   ],
   "source": [
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "# prepare the explainer\n",
    "cls_explainer = CustomSequenceClassificationExplainer(model, tokenizer)\n",
    "\n",
    "# run the explainer on an example\n",
    "example_sentence = \"My parents hated this ridiculous movie.\"\n",
    "\n",
    "word_attributions = cls_explainer(example_sentence)\n",
    "\n",
    "# the explainer outputs the attributions for each sub-token in the input\n",
    "print(\"Sub-token attributions:\", word_attributions)\n",
    "\n",
    "# the attributions are specifically targetting the predicted class\n",
    "print(\"Predicted class:\",cls_explainer.predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00caa97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 0.0\n",
      "‚ñÅMy 0.0023662683176213025\n",
      "‚ñÅparents -0.0044839087437695165\n",
      "‚ñÅhat 0.13700055463100105\n",
      "ed 0.838185384402232\n",
      "‚ñÅthis 0.4072319378334896\n",
      "‚ñÅridiculous 0.2140027340738356\n",
      "‚ñÅmovie 0.12540142859780076\n",
      "‚ñÅ 0.1389692863864906\n",
      ". 0.1788222651295735\n",
      "</s> 0.0\n"
     ]
    }
   ],
   "source": [
    "# you can separate the sub-tokens and attributions by zipping them\n",
    "sub_tokens, attributions = zip(*word_attributions)\n",
    "\n",
    "# you can print them out together again like this\n",
    "for sub_token, attribution in zip(sub_tokens, attributions):\n",
    "    print(sub_token, attribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c93eaa",
   "metadata": {},
   "source": [
    "These attributions still need some work, as some of them are assigned to empty tokens.\n",
    "In addition, some emoji's or special character are slit into different sub-tokens\n",
    "For that purpose, we provide the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "238cfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_AttributionTokens(tokenized_text, attributions):\n",
    "    \"\"\"creates a vector of binary values to indicate whether a word is a trigger word or not (based on a predefined threshold)\n",
    "\n",
    "    Args:\n",
    "        normalized_scores (list): a list of numerical scores that have already been normalized (i.e., they sum to 1.0)\n",
    "        threshold (float, optional): A lower bound for converting numerical values to a binary 1. Values below the threshold are converted to 0. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        list: a binary vector of 1s and 0s indicating whether a word is a trigger word or not\n",
    "    \"\"\"\n",
    "    offset_mapping = tokenizer(tokenized_text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "    #print(offset_mapping)\n",
    "    final_attributions = {}\n",
    "\n",
    "    # to ensure the same mapping, we need to find the indices of the spaces (which are token+1)\n",
    "    space_indices = [] # counting first character as a space because otherwise the first token will be skipped\n",
    "\n",
    "    # keep track of the space indices\n",
    "    for char_index, character in enumerate(tokenized_text + \" \"):  # add a space to capture the final token\n",
    "        #print(char_index, character)\n",
    "        if character.isspace():\n",
    "            #print(\"Space found\", char_index)\n",
    "            space_indices.append(char_index)\n",
    "\n",
    "    # not very effective to run over ALL mappings for EACH token, but it works\n",
    "    # for each space (i.e. token), find the corresponding sub-tokens and sum the attributions based on character\n",
    "    for i, space_index in enumerate(space_indices, start =0):\n",
    "        final_attributions[i] = 0\n",
    "        for tokenindex, mapping in enumerate(offset_mapping):\n",
    "            begin_index = mapping[0]\n",
    "            end_index = mapping[1]\n",
    "            if begin_index == 0 and end_index == 0: # ignore BoS and EoS tokens\n",
    "                continue\n",
    "            elif i == 0: # special treatment because there is no previous token for the first token\n",
    "                if space_index >= end_index: # any sub-tokens before space index (token delimiter) are concatenated \n",
    "                    final_attributions[i] += attributions[tokenindex]\n",
    "            \n",
    "            else:\n",
    "                if space_index >= end_index and begin_index >= space_indices[i-1]: # begin index > previous space index because otherwise importances will overlap\n",
    "                    final_attributions[i] += attributions[tokenindex]\n",
    "                elif space_index < begin_index:\n",
    "                    break\n",
    "    \n",
    "    #print(final_attributions)\n",
    "    final_outputs = []\n",
    "    for key in final_attributions.keys():\n",
    "        final_outputs.append(final_attributions[key])\n",
    "    return tokenized_text.split(\" \"), final_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa37e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "<s> 0.0\n",
      "‚ñÅMy 0.0023662683176213025\n",
      "‚ñÅparents -0.0044839087437695165\n",
      "‚ñÅhat 0.13700055463100105\n",
      "ed 0.838185384402232\n",
      "‚ñÅthis 0.4072319378334896\n",
      "‚ñÅridiculous 0.2140027340738356\n",
      "‚ñÅmovie 0.12540142859780076\n",
      "‚ñÅ 0.1389692863864906\n",
      ". 0.1788222651295735\n",
      "</s> 0.0\n",
      "After\n",
      "My 0.0023662683176213025\n",
      "parents -0.0044839087437695165\n",
      "hated 0.975185939033233\n",
      "this 0.4072319378334896\n",
      "ridiculous 0.2140027340738356\n",
      "movie. 0.2643707149842913\n"
     ]
    }
   ],
   "source": [
    "print('Before')\n",
    "# you can print them out together again like this\n",
    "for sub_token, attribution in zip(sub_tokens, attributions):\n",
    "    print(sub_token, attribution)\n",
    "\n",
    "final_tokens, new_attributions = Clean_AttributionTokens(tokenized_text=example_sentence, attributions=attributions)\n",
    "print(\"After\")\n",
    "for token, attribution in zip(final_tokens, new_attributions): \n",
    "    print(token, attribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb309ff2",
   "metadata": {},
   "source": [
    "As you can see, these attributions include negatives and positives. Negative attributions may make some sense for a binary classification task, but for multi-class classification, it is hard to know what that means.\n",
    "Also, the total attribution for sentences does not necessarily add up to 1 or any specific value.\n",
    "For our task, we normalize the scores per sentence so they have equal weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f78570ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_Attributions(attributions):\n",
    "    \"\"\" Function to normalize attributions to sum to 1, ignoring negative attributions\n",
    "\n",
    "    Args:\n",
    "        attributions (list): numerical attribution scores for each sub-token as theiy are split by the transformer tokenizer\n",
    "\n",
    "    Returns:\n",
    "        list: normalized attributions that sum up to 1 for each sentence\n",
    "    \"\"\"\n",
    "    added_non_negatives = 0\n",
    "    non_negatives = []\n",
    "    for attributionscore in attributions:\n",
    "        if float(attributionscore) < 0:\n",
    "            nonzeroscore = 0\n",
    "        else:\n",
    "            nonzeroscore = float(attributionscore)\n",
    "            added_non_negatives += float(attributionscore)\n",
    "        non_negatives.append(nonzeroscore)\n",
    "\n",
    "    if added_non_negatives != 0: # if added values are zero, crashes due to zero division\n",
    "        relative_contributions = [non_negative/added_non_negatives for non_negative in non_negatives] # default route\n",
    "    else:\n",
    "        relative_contributions = [0 for non_negative in non_negatives] # backup for zero division\n",
    "    return relative_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "282b9b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My 0.001270031222765881\n",
      "parents 0.0\n",
      "hated 0.5234049669479127\n",
      "this 0.218570849343028\n",
      "ridiculous 0.11486024302782911\n",
      "movie. 0.1418939094584643\n"
     ]
    }
   ],
   "source": [
    "# using this function, we can normalize the attributions\n",
    "new_attributions = Normalize_Attributions(attributions=new_attributions)\n",
    "for token, attribution in zip(final_tokens, new_attributions): # get rid of <s> and </s> tokens\n",
    "    print(token, attribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba798fc0",
   "metadata": {},
   "source": [
    "For one part of the Task, we evaluate on numerical values, meaning you do not need this step/function. However, for the other, we only take binary trigger word indicators.\n",
    "This means that the numerical values have to be converted to binary values. This can be done with the following function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a520ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateBinaryVector(normalized_scores, threshold=0.2):\n",
    "    \"\"\"creates a vector of binary values to indicate whether a word is a trigger word or not (based on a predefined threshold)\n",
    "\n",
    "    Args:\n",
    "        normalized_scores (list): a list of numerical scores that have already been normalized (i.e., they sum to 1.0)\n",
    "        threshold (float, optional): A lower bound for converting numerical values to a binary 1. Values below the threshold are converted to 0. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        list: a binary vector of 1s and 0s indicating whether a word is a trigger word or not\n",
    "    \"\"\"\n",
    "    normalized_scores = normalized_scores\n",
    "    vector = []\n",
    "    for attribution in normalized_scores:\n",
    "        if attribution >= threshold:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1967bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My 0\n",
      "parents 0\n",
      "hated 1\n",
      "this 1\n",
      "ridiculous 1\n",
      "movie. 1\n"
     ]
    }
   ],
   "source": [
    "binary_vector = CreateBinaryVector(new_attributions, threshold=0.1)\n",
    "for token, attribution in zip(final_tokens, binary_vector):\n",
    "    print(token, attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9e433c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My 0\n",
      "parents 0\n",
      "hated 1\n",
      "this 1\n",
      "ridiculous 1\n",
      "movie. 1\n"
     ]
    }
   ],
   "source": [
    "# this can be combined into a single function\n",
    "# BUT: beware that you may need additional cleaning steps or token merging depending on your tokenizer\n",
    "\n",
    "def Vector_from_raw_attributions(inputstring, interpret_output, threshold=0.1):\n",
    "    \"\"\" a combined function to normalize, clean, and create a binary vector from raw numerical attributions values for subtokens\n",
    "    Args:\n",
    "        inputstring (string): the input text string (from the column [\"Texts\"])\n",
    "        interpret_output (Tuple): a Tuple containing the output from the importance attribution model (i.e., the raw attributions + the subtokens)\n",
    "        threshold (float, optional): Minimal required importance to convert the numerical value to a binary 1. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        list: a vector of binary values indicating whether a token is a trigger word or not\n",
    "    \"\"\"\n",
    "    sub_tokens, attribute_scores = zip(*interpret_output)\n",
    "    tokenized_sample, attribute_scores = Clean_AttributionTokens(inputstring, attribute_scores)\n",
    "    attribute_scores = Normalize_Attributions(attributions=attribute_scores)\n",
    "    attribute_scores = CreateBinaryVector(attribute_scores, threshold)\n",
    "    return tokenized_sample, attribute_scores\n",
    "\n",
    "example_sentence = \"My parents hated this ridiculous movie.\"\n",
    "interpret_output = cls_explainer(example_sentence)\n",
    "\n",
    "tokenized_sample, final_vector = Vector_from_raw_attributions(example_sentence, interpret_output)\n",
    "\n",
    "for token, attribution in zip(tokenized_sample, final_vector): # get rid of <s> and </s> tokens\n",
    "    print(token, attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8aac53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID                                              Texts  \\\n",
      "0      7168  @user I‚Äôm so happy you‚Äôve found some success f...   \n",
      "1     11762  @user Awww , thank you üòö Well , nobody knows ....   \n",
      "2     10854  @user Heheh . Me too actually ! Welcome ( back...   \n",
      "3      7294  @user Hey hun just thought I‚Äôd show you how vi...   \n",
      "4     10435  @user given the dreadful performance of my bat...   \n",
      "...     ...                                                ...   \n",
      "2995   7906  there isnt a day where im not deeply upset abo...   \n",
      "2996   8809  @user Luckily , thanks to an old friend of min...   \n",
      "2997   8031  I feel very lost and not sure about what Im doing   \n",
      "2998   7456  Im gonna read the Captain Phasma novel and hop...   \n",
      "2999   8783                      @user Ya its been tooooo long   \n",
      "\n",
      "                                                 Labels  \n",
      "0                        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0]  \n",
      "1         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
      "2         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
      "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
      "4     [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                 ...  \n",
      "2995  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
      "2996  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2997                  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
      "2998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2999                                 [0, 0, 0, 0, 1, 0]  \n",
      "\n",
      "[3000 rows x 3 columns]\n",
      "0 @user I‚Äôm so happy you‚Äôve found some success for yourself\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 1, 0]\n",
      "True\n",
      "1 @user Awww , thank you üòö Well , nobody knows . . . üòÇ üòú\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# to run this on the entire dataset\n",
    "import pandas as pd\n",
    "\n",
    "# pick the model you want to use\n",
    "MODEL_NAME = \"pranaydeeps/EXALT-Baseline\"\n",
    "\n",
    "# load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "cls_explainer = CustomSequenceClassificationExplainer(model, tokenizer)\n",
    "\n",
    "traindata = pd.read_csv(\"data/exalt_trigger_train.tsv\", sep=\"\\t\")\n",
    "print(traindata)\n",
    "\n",
    "predictions = []\n",
    "for rowindex, row in traindata[:2].iterrows():\n",
    "    tweet_text = row[\"Texts\"]\n",
    "    print(rowindex, tweet_text)\n",
    "    interpret_output = cls_explainer(tweet_text)\n",
    "    final_tokens, final_vector = Vector_from_raw_attributions(tweet_text, interpret_output)\n",
    "    print(final_vector)\n",
    "    print(len(final_vector) ==len(tweet_text.split(\" \")))\n",
    "    predictions.append(final_vector)\n",
    "\n",
    "\n",
    "traindata = traindata.iloc[:2]\n",
    "traindata[\"Labels\"] = predictions # MAKE SURE TO NAME THIS COLUMN \"Labels\" FOR THE EVALUATION SCRIPT TO WORK\n",
    "traindata.to_csv(\"data/exalt_trigger_train_predictions.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
